[
  "Pre-training an LLM model from scratch refers to the process of training a language model on a large corpus of data (e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-tuning, where an already pre-trained model is further adapted to a specific task or data set. The output of full pre-training is a base model that can be directly used or further fine-tuned for downstream tasks. Pre-training is typically the largest and most expensive training tasks one would encounter, and not typical for what most organizations would undertake.",
  "Pre-training an LLM model from scratch refers to the process of training a language model on a large corpus of data (e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-tuning, where an already pre-trained model is further adapted to a specific task or data set. The output of full pre-training is a base model that can be directly used or further fine-tuned for downstream tasks. Pre-training is typically the largest and most expensive training tasks one would encounter, and not typical for what most organizations would undertake.",
  "What is LLM (Large Language Model)?",
  "What is LLM (Large Language Model)?",
  "Now that we can predict one word, we can feed the extended sequence back into the LLM and predict another word, and so on. In other words, using our trained LLM, we can now generate text, not just a single word. This is why LLMs are an example of what we call Generative AI. We have just taught the LLM to speak, so to say, one word at a time.",
  "This is where instruction tuning comes in. We take the pre-trained LLM with its current abilities and do essentially what we did before — i.e., learn to predict one word at a time — but now we do this using only high-quality instruction and response pairs as our training data.",
  "The first is what we have already brought up: The LLM may just hallucinate and simply respond with a wrong or even fake name.",
  "In the image above you can see what a typical prompt for an LLM with additional context may look like. (By the way, prompt is just another name for the instructions we give to an LLM, i.e., the instructions form the input sequence.)",
  "Note how we simply left out the solution to the last example. Remember that an LLM is still a text-completer at heart, so keep a consistent structure. You should almost force the model to respond with just what you want, as we did in the example above.",
  "Always improving: Large language model performance is continually improving because it grows when more data and parameters are added. In other words, the more it learns, the better it gets. What’s more, large language models can exhibit what is called \"in-context learning.\" Once an LLM has been pretrained, few-shot prompting enables the model to learn from the prompt without any additional parameters. In this way, it is continually learning.",
  "Hallucinations: A hallucination is when a LLM produces an output that is false, or that does not match the user's intent. For example, claiming that it is human, that it has emotions, or that it is in love with the user. Because large language models predict the next syntactically correct word or phrase, they can't wholly interpret human meaning. The result can sometimes be what is referred to as a \"hallucination.\"",
  "Always improving: Large language model performance is continually improving because it grows when more data and parameters are added. In other words, the more it learns, the better it gets. What’s more, large language models can exhibit what is called \"in-context learning.\" Once an LLM has been pretrained, few-shot prompting enables the model to learn from the prompt without any additional parameters. In this way, it is continually learning.",
  "Hallucinations: A hallucination is when a LLM produces an output that is false, or that does not match the user's intent. For example, claiming that it is human, that it has emotions, or that it is in love with the user. Because large language models predict the next syntactically correct word or phrase, they can't wholly interpret human meaning. The result can sometimes be what is referred to as a \"hallucination.\""
]